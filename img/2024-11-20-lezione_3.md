---
layout: post
title: "Lezione 3: Iperparametri"
description: "Tutti conoscono gli ingredienti per una torta, ma pochi sanno dosarli con maestria"
media_subpath: /img/lesson_3
image:
  path: /lesson_3.jpg
date: 2024-11-20
math: true
categories: [Machine Learning]
tags: [machine-learning, introduction, learning-algorithms, data-science, ai-basics, model-training, ml-theory]     # TAG names should always be lowercase
---
{% include disclaimer.html %}

Nella **[scorsa lezione](../lezione_2)**, abbiamo identificato gli ingredienti fondamentali per costruire una learning machine: una funzione di predizione, una funzione di perdita e un dataset. Questi componenti ci permettono di formalizzare matematicamente la nostra prima macchina d'apprendimento.

Tuttavia, definire le funzioni non è sufficiente. Dobbiamo trovare i parametri ottimali che permettono alla nostra macchina di fare previsioni accurate: i cosiddetti **iperparametri**.

<blockquote class="prompt-tip">
{% raw %}
Gli ingredienti per la nostra prima macchina di apprendimento:<br><br>
1. <strong>Funzione di Predizione</strong>: <div id="eq-polynomial"> 
$$
\hat{y} = f(x) = \sum_{i=0}^p c_i x^i \tag{1}
$$
</div>
2. <strong>Funzione di Perdita</strong>: <div id="eq-loss">
$$
l(f(x), y) = (y - f(x))^2 \tag{2}
$$
</div>
3. <strong>Dataset</strong>: $$ D_n = \{ (x_1, y_1), \dots, (x_n, y_n) \} $$

{% endraw %}
</blockquote>

<blockquote class="prompt-warning">
{% raw %}
\( f \) e \( l \) li abbiamo scelti.<br> \( p \) e \( c_i \) sono da trovare.
{% endraw %}
</blockquote>

Le variabili che dobbiamo trovare sono dette **iperparametri**.

$$p$$ e $$c_i$$ sono iperparametri.

Impariamo a conoscerli.

<h3 id="p"><span class="me-2">\(p\) </span><a href="#p" class="anchor text-muted"></a></h3>

Immaginiamo una situazione del genere:

![Light mode only](/light_no_line.png){: .light }
![Dark mode only](/dark_no_line.png){: .dark }

Qual è il modello che più approssima questa serie di punti?

Un cerchio?

Una parabola?

O cos'altro?

Prima di proseguire, prova a rispondere!

![Light mode only](/linear_regression_plot_light_mode.png){: .light }
![Dark mode only](/linear_regression_plot_dark.png){: .dark }

<blockquote class="prompt-tip"> Se hai risposto <strong>retta</strong>, complimenti: non sei un robot! </blockquote>

Per noi esseri umani è evidente che il modello che più approssima questa serie di punti è una retta.

Per trovare $$p$$, che ti ricordo rappresenta il grado della funzione, ovvero la sua complessità, ci abbiamo messo un attimo, il tempo di un'occhiata.

Questo perché l'intelligenza è una caratteristica prettamente umana, tuttavia siamo estremamente lenti a fare i calcoli. Ti sfido infatti a trovare $$m$$, il coefficiente angolare della retta di cui hai indovinato il modello istantaneamente. Dovremmo calcolare una retta per ogni coppia di punti e minimizzare l’errore per ciascuna di esse per poi scegliere la retta con l'errore minimo complessivo. In poche parole impazziremmo!

Per i computer, funziona esattamente al contrario: sono stupidi e non hanno intuizioni, ma sono estremamente veloci. Trovare parametri come $$m$$ e $$c_i$$ è un gioco da robot per loro, mentre altri parametri, come $$p$$, sono molto complessi da determinare.

L'idea alla base del machine learning è proprio questa: trasformare tutto quello che è intelligente in un problema risolvibile con la forza bruta e con molta potenza computazionale.

<blockquote class="prompt-info">{% raw %} Per un computer alcuni parametri sono difficili da trovare, ad esempio  \( p \), mentre altri sono facili, come  \( c_i \). {% endraw %}</blockquote>

Vediamo perché per un computer è facile trovare $$c_i$$.

<h3 id="c"><span class="me-2">\(c_i\) </span><a href="#c" class="anchor text-muted"></a></h3>

Riprendiamo l'equazione [$$ (1) $$](#eq-polynomial), ovvero la nostra funzione di predizione:

$$
\hat{y} = f(x) = \sum_{i=0}^p c_i x^i
$$

e supponiamo che $$ p $$ ci sia fornita da un oracolo. Vogliamo trovare il valore ottimale di $$ c_i $$.

In altri termini, vogliamo individuare la funzione $$f(x)$$ migliore tra il set delle possibili $$f(x)$$, poiché il cambiamento di $$c_i$$ comporta il cambiamento della funzione $$f(x)$$.

Come facciamo?

Il nostro obiettivo è determinare il valore di $$c_i$$ che minimizza l'errore medio sui dati a nostra disposizione (ovvero voglio fare in modo che le previsioni della funzione che scelgo siano il più vicine possibile ai valori reali).

Questo equivale a dire:

$$
\min_{\underline{c}} \frac{1}{n} \sum_{i=1}^{n} \left( y_i - f(x_i) \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \sum_{j=0}^{p} c_j x_i^j \right)^2
$$

Quello che ho fatto è prendere la [$$ (2) $$](#eq-loss), ovvero la funzione di perdita, sostituire l'espressione $$ f(x) $$ con la funzione di predizione e minimizzare rispetto a $$ c_i $$ che è il vettore che contiene tutti i $$(p + 1)$$ parametri.

Durante questa procedura, possiamo escludere il fattore $$\frac{1}{n}$$ dall'errore medio. Infatti, questa costante moltiplicativa non influisce sul risultato della minimizzazione. Quindi:

$$
\min_{\underline{c}} \sum_{i=1}^{n} \left( y_i - f(x_i) \right)^2 = \sum_{i=1}^{n} \left( y_i - \sum_{j=0}^{p} c_j x_i^j \right)^2 =
$$

<div id="eq-matrix">
$$
=\|X \underline{c} - \underline{y}\|^2 \tag{3}
$$
</div>

Per convincerci di quest'ultima risultato basta definire:

$$
\underline{c} = 
\begin{bmatrix}
c_0 \\
\vdots \\
c_p
\end{bmatrix} 
\in \mathbb{R}^{p+1}
$$

$$
\underline{y} = 
\begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix} 
\in \mathbb{R}^n
$$

$$
X = 
\begin{bmatrix}
x_1^0 & \cdots & x_1^p \\
\vdots & \ddots & \vdots \\
x_n^0 & \cdots & x_n^p
\end{bmatrix} 
\in \mathbb{R}^{n \times (p+1)}
$$

ed esplicitare i calcoli:

$$
\|X \underline{c} - \underline{y}\|^2 = 
\left\|
\begin{bmatrix}
x_1^0 & x_1^1 & \cdots & x_1^p \\
\vdots & \vdots & \ddots & \vdots \\
x_i^0 & x_i^1 & \cdots & x_i^p \\
\vdots & \vdots & \ddots & \vdots \\
x_n^0 & x_n^1 & \cdots & x_n^p
\end{bmatrix}
\begin{bmatrix}
c_0 \\
c_1 \\
\vdots \\
c_p
\end{bmatrix}
-
\begin{bmatrix}
y_1 \\
\vdots \\
y_i \\
\vdots \\
y_n
\end{bmatrix}
\right\|^2
$$

Adesso, focalizziamoci per esempio sull'elemento $$i$$-esimo della norma. Dopo aver moltiplicato la $$i$$-esima riga di $$X$$ con $$\underline{c}$$ e aver sottratto $$y_i$$, otteniamo:

$$
\left( 
\begin{bmatrix}
x_i^0 & x_i^1 & \cdots & x_i^p
\end{bmatrix}
\begin{bmatrix}
c_0 \\
c_1 \\
\vdots \\
c_p
\end{bmatrix}
- y_i 
\right)^2
= 
\left( \sum_{j=0}^{p} c_j x_i^j - y_i \right)^2
$$

Quindi, il termine $$i$$-esimo all'interno della norma quadrata è:

$$
\|X \underline{c} - \underline{y}\|^2 = \sum_{i=1}^{n} \left( y_i - \sum_{j=0}^{p} c_j x_i^j \right)^2
$$

che è esattamente la [$$ (3) $$](#eq-matrix). Voilà!

Quindi devo minimizzare questa quantità:

$$
\min_{\underline{c}} \|X \underline{c} - \underline{y}\|^2
$$

Per affrontare questo problema, dobbiamo prima comprendere la struttura matematica della funzione da minimizzare.

La funzione $$ \|X \underline{c} - \underline{y}\|^2 $$ è il quadrato di una norma, che in termini più semplici può essere visto come una generalizzazione della forma quadratica $$ ax^2 + bx + c $$, ma in più dimensioni.

Graficamente si tratta di una parabola generalizzata nello spazio multidimensionale, nota come **paraboloide**.

![Light mode only](/paraboloid_light.png){: .light }
![Dark mode only](/paraboloid_dark.png){: .dark }
_Un paraboloide. Affascinante, vero?_

Questa struttura ci piace perché, oltre ad essere affascinante, ha una caratteristica molto utile: esiste un unico punto di minimo globale. In una dimensione, per trovare il minimo di una funzione quadratica, imposteremmo la derivata prima a zero.

In modo analogo, in più dimensioni, utilizziamo il **gradiente**: poniamo il gradiente della funzione a zero per individuare il punto in cui si raggiunge il minimo. Facciamolo insieme:

$$
\nabla_{\underline{c}} \left( \underline{c}'X'X\underline{c} - 2\underline{c}'X'\underline{y} + \underline{y}'\underline{y} \right) = \underline{0}
$$

$$
\cancel{2}X'X\underline{c} - \cancel{2}X'\underline{y} = \underline{0}
$$

$$
X'X\underline{c} = X'\underline{y}
$$

Ora, per ottenere $$c_i$$ abbiamo due opzioni.

1. Possiamo considerare &nbsp; $$ X'X\underline{c} = X'\underline{y} $$ &nbsp; come un un sistema lineare nella forma $$ A \underline{x} = \underline{b} $$
2. Oppure possiamo risolvere:

$$
\underline{c} = (X'X)^+ X'\underline{y}
$$

dove $$(X'X)^+$$ è la generalizzazione della matrice inversa che si chiama **pseudoinversa**.

<blockquote class="prompt-info">
  <p><strong>La pseudoinversa</strong> è uno strumento matematico che si usa per risolvere sistemi di equazioni lineari quando non possiamo calcolare la normale inversa di una matrice.</p>

  <p>Quando può succedere?</p>

  <ol>
    <li>
      <strong>Matrice non quadrata.</strong>
      <ul>
        <li>Questo accade, ad esempio, quando ci sono più variabili che equazioni, ovvero i dati sono limitati rispetto alla complessità del modello.</li>
      </ul>
      <p>In questi casi, il sistema ha infinite soluzioni e la pseudoinversa sceglie la soluzione più semplice, cioè quella con i valori più piccoli.</p>
    </li>
    <li>
      <strong>Matrice quadrata, ma singolare.</strong>
      <ul>
        <li>Questo accade, ad esempio, quando ci sono più equazioni che variabili, ovvero ci sono più dati o osservazioni rispetto al numero di variabili o parametri che stiamo cercando di stimare.</li>
      </ul>
      <p>In questo caso, non esiste una soluzione che soddisfi tutte le equazioni e la pseudoinversa trova la migliore soluzione possibile, cioè quella che minimizza l’errore tra i valori previsti e quelli reali.</p>
    </li>
  </ol>
</blockquote>

#### Caso 1: risoluzione di un singolo sistema lineare
Nel primo caso stiamo considerando il sistema:

$$ X'X\underline{c} = X'\underline{y} $$

e stiamo dicendo che può essere rappresentato come:

$$ A \underline{x} = \underline{b} $$

dove:

- $$ X'X $$ rappresenta la matrice dei coefficienti $$ A $$.
- $$ X'y $$ è il vettore dei termini noti $$ \underline{b} $$.
- $$ \underline{c} $$ è il vettore delle incognite $$ \underline{x} $$.

Proviamo a calcolarne la complessità, partendo da un sistema lineare il più semplice possibile, ad esempio:

$$
\begin{aligned}
3x + 4y &= 7 \\
7x + 3y &= 4
\end{aligned}
$$

In forma matriciale:

$$
\begin{bmatrix}
3 & 4 \\
7 & 3
\end{bmatrix}
\cdot
\begin{bmatrix}
x \\
y
\end{bmatrix}
=
\begin{bmatrix}
7 \\
4
\end{bmatrix}
$$

Dove:

$$
A =
\begin{bmatrix}
3 & 4 \\
7 & 3
\end{bmatrix}
\quad
x =
\begin{bmatrix}
x \\
y
\end{bmatrix}
\quad
b =
\begin{bmatrix}
7 \\
4
\end{bmatrix}
$$

Per risolvere il sistema possiamo procedere tramite eliminazione di Gauss.

Sottraiamo $$\frac{7}{3}$$ volte la prima riga dalla seconda e la nuova seconda riga diventa:
   
   $$
   0 + \left(-\frac{7}{3} \cdot 4 + 3\right)y = 4 - \frac{7}{3} \cdot 7
   $$

In forma matriciale otteniamo quindi:

$$
   \begin{bmatrix}
   3 & 4 \\
   0 & -\frac{1}{3}
   \end{bmatrix}
$$

In generale, se immaginiamo $$ A $$ come una matrice più grande, quello che abbiamo appena ottenuto sarebbe qualcosa di simile a questo:

$$
A =
\begin{bmatrix}
. &   &   &   &   &   \\
0 & . &   &   &   &   \\
. & . & . &   &   &   \\
. & . & . & . &   &   \\
. & . & . & . & . &   \\
. & . & . & . & . & .
\end{bmatrix}
$$

E possiamo rendere zeri tutti i numeri sotto lo zero appena ottenuto:

$$
A =
\begin{bmatrix}
. &   &   &   &   &   \\
0 & . &   &   &   &   \\
0 & . & . &   &   &   \\
0 & . & . & . &   &   \\
0 & . & . & . & . &   \\
0 & . & . & . & . & .
\end{bmatrix}
$$

E poi procedere con la seconda colonna sotto la diagonale principale:

$$
A =
\begin{bmatrix}
. &   &   &   &   &   \\
0 & . &   &   &   &   \\
0 & 0 & . &   &   &   \\
0 & 0 & . & . &   &   \\
0 & 0 & . & . & . &   \\
0 & 0 & . & . & . & .
\end{bmatrix}
$$

Fino ad ottenere questa:

$$
A =
\begin{bmatrix}
. &   &   &   &   &   \\
0 & . &   &   &   &   \\
0 & 0 & . &   &   &   \\
0 & 0 & 0 & . &   &   \\
0 & 0 & 0 & 0 & . &   \\
0 & 0 & 0 & 0 & 0 & .
\end{bmatrix}
$$

Dall'ultima riga posso ottenere un'equazione di primo grado nella forma:

$$
x = \frac{b}{a}
$$

Quest'ultima soluzione la posso mettere nella penultima riga, ottenendo un'altra equazione di primo grado nella stessa forma, e così via fino in cima.

Troviamo così la soluzione del sistema.

Per una matrice $$n \times n $$, la complessità computazionale è 
$$
O(n^2)
$$ 
perché devo trasformare in zeri metà della matrice che è 
$$
n \times n = n^2
$$ 
e, dividendo per $$2$$, otteniamo 
$$
\frac{n^2}{2}
$$ 
da cui 
$$
O(n^2).
$$


#### Caso 2: risoluzione tramite pseudo-inversa

Ora, consideriamo il caso più complesso in cui vogliamo risolvere il sistema usando la pseudo-inversa:

$$
\underline{c} = (X'X)^+ X'\underline{y}
$$


Qui stiamo cercando una soluzione ottimale valida per **tutti i possibili vettori** $$ \underline{y} $$.
Stiamo cioè analizzando l'intero spazio delle soluzioni per la matrice $$A = X'X$$ trovando la soluzione migliore per ogni possibile $$b$$.

Per queste ragioni calcolare la pseudo-inversa ha una complessità computazionale di $$O(n^3)$$, superiore rispetto a quella del primo metodo perché sto processando più informazioni e, naturalmente, se faccio qualcosa di più complesso, completo, informativo dobbiamo pagare di più in termini di memoria e potenza di calcolo.

---

Per riassumere:
<blockquote class="prompt-tip">
<strong>Caso 1: Risoluzione tramite metodo diretto</strong>  
<div>
$$
X'X\underline{c} = X'\underline{y}
$$
</div>
rappresentabile come:  
<div>
$$
A \underline{x} = \underline{b}
$$
</div>
<ul>
  <li><strong>Tipo di soluzione</strong>: Trova una soluzione specifica per un singolo sistema lineare.</li>
  <li><strong>Complessità computazionale</strong>: 
  $$
  O(n^2)
  $$  
  in quanto riduce in zeri metà degli elementi sotto la diagonale principale.</li>
</ul>

<strong>Caso 2: Risoluzione tramite pseudo-inversa</strong>  
<div>
$$
\underline{c} = (X'X)^+ X'\underline{y}
$$
</div>
<ul>
  <li><strong>Tipo di soluzione</strong>: Trova una soluzione ottimale valida per tutti i possibili vettori \( \underline{y} \), analizzando l'intero spazio delle soluzioni.</li>
  <li><strong>Complessità computazionale</strong>: 
  $$
  O(n^3)
  $$  
  dovuta al calcolo della pseudo-inversa, più costoso in termini di memoria e potenza di calcolo.</li>
</ul>

</blockquote>

Siamo quindi riusciti a trovare $$c_i$$. Avendo i dati, la funzione predittiva e la funzione di perdita, dato $$ p $$, possiamo scrivere il codice che trova i migliori coefficienti!

Non è favoloso? Per un robot lo è sicuramente!

Con questa lezione abbiamo avuto un assaggio di cosa siano gli iperparametri e come assumino un ruolo fondamentale nel complesso compito dell’apprendimento. Tuttavia, sapere come trovare i parametri è solo l'inizio.

Nella prossima lezione ci addentreremo nel caos della realtà: scopriremo perché i dati non seguono sempre le nostre regole e come la complessità del mondo influenzi il nostro modo di progettare i modelli. Stiamo raschiando la superficie di un mondo sommerso che non aspetta altro che essere scoperto.

Alla prossima!